<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://secrul.github.io</id>
    <title>secrul</title>
    <updated>2020-09-23T05:22:18.831Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://secrul.github.io"/>
    <link rel="self" href="https://secrul.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://secrul.github.io/images/avatar.png</logo>
    <icon>https://secrul.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, secrul</rights>
    <entry>
        <title type="html"><![CDATA[FRACTALNET]]></title>
        <id>https://secrul.github.io/post/fractalnet/</id>
        <link href="https://secrul.github.io/post/fractalnet/">
        </link>
        <updated>2020-09-21T12:53:27.000Z</updated>
        <content type="html"><![CDATA[<p>分形：一种数学几何概念。一种局部与全局有着结构相似性的几何图形。本文表明，残差表示不是极深卷积神经网络成功的基本要素。设计了一种分形，无残差的网络。<br>
<img src="https://secrul.github.io/post-images/1600783963806.png" alt="" loading="lazy"><br>
红色是卷积，绿色可以是相加或者concat，但是此处是求加权和，通道数仍然相同。</p>
<p>本文同样参考了路径drop来减少计算量。<br>
<img src="https://secrul.github.io/post-images/1600784049681.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Densely Connected Convolutional Networks]]></title>
        <id>https://secrul.github.io/post/densely-connected-convolutional-networks/</id>
        <link href="https://secrul.github.io/post/densely-connected-convolutional-networks/">
        </link>
        <updated>2020-09-21T12:53:09.000Z</updated>
        <content type="html"><![CDATA[<p>贡献：<br>
1.缓解了梯度消失的问题<br>
2.增强了特征后向传播<br>
3.<strong>特征的再利用</strong><br>
4.极大地减少了参数量</p>
<p><img src="https://secrul.github.io/post-images/1600751268381.png" alt="" loading="lazy"><br>
借鉴了ResNet的思想，如图中所示，每一个layer与其后所有的layer都有一个连接，这也是dense的由来。但是此处是concatenate，而不是ResNet中的简单相加。这个方法极大的实现了特征的再利用，也缓解了梯度消失的问题。并且与ResNet相比，性能相似的时候网络的深度大大减少，参数数量也大大减少。<br>
<img src="https://secrul.github.io/post-images/1600751772420.png" alt="" loading="lazy"><br>
显示的是DenseNet在实际的应用，借鉴了ResBlock的概念，在每一个denseblock中的特征图的大小都是相同的，可以直接在channel上进行concat。<br>
生长率Growth rate：如果denseblock的卷积是conv(in, out=k,stride,padding)，即每一个卷积都输出k个特征图，那么在第l层的卷积的输入就是(l - 1) * k + k0,输入就会很大，引入denseblock也是为了增大网络的深度，减少输入的channel数目。</p>
<p>DenseBlock之间的非线性组合函数H包括BN+ReLU+3x3 Conv，为了进一步减少输入的channel，减少计算量，引入了1x1的卷积。中间过渡层H函数变为了BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv，称为DenseNet-B结构。<br>
假定Transition的上接DenseBlock得到的特征图channels数为 m ，Transition层可以产生 [<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">\theta m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mord mathdefault">m</span></span></span></span>] 个特征（通过卷积层），其中 [<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>] 是压缩系数（compression rate）。当 [<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> = 1] 时，特征个数经过Transition层没有变化，即无压缩，而当压缩系数小于1时，这种结构称为DenseNet-C，文中使用 [<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span>] 。对于使用bottleneck层的DenseBlock结构和压缩系数小于1的Transition组合结构称为DenseNet-BC。<br>
denseNet相比ResNet有着更少的参数，而在参数数目相同的情况下，性能有好于ResNet，但是计算过程中会消耗更多的显存，并没有ResNet知名。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Networks with Stochastic Depth]]></title>
        <id>https://secrul.github.io/post/deep-networks-with-stochastic-depth/</id>
        <link href="https://secrul.github.io/post/deep-networks-with-stochastic-depth/">
        </link>
        <updated>2020-09-21T12:52:41.000Z</updated>
        <content type="html"><![CDATA[<p>借鉴了Drop何ResNet的思想，不过Drop是随机去除节点来减少网络的宽度。而本论文中是随机去除Block，来减少网络的深度。<br>
<img src="https://secrul.github.io/post-images/1600773009458.png" alt="" loading="lazy"><br>
Dropout(p)，假设p符合伯努利分布，以P的概率舍弃一个节点。而本文中以p的概率跨过一个Block。<br>
<img src="https://secrul.github.io/post-images/1600773270477.png" alt="" loading="lazy"><br>
没什么难理解的，发现效果要比固定深度的网络要好</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[角点检测算子——Moravec、Harris、SUSAN]]></title>
        <id>https://secrul.github.io/post/jiao-dian-jian-ce-suan-zi-moravecharrissusan/</id>
        <link href="https://secrul.github.io/post/jiao-dian-jian-ce-suan-zi-moravecharrissusan/">
        </link>
        <updated>2020-09-12T06:57:41.000Z</updated>
        <content type="html"><![CDATA[<p>什么是角点：<br>
如果一个点在其小邻域中具有两个明显不同的边缘方向，称之为角点。也有人将局部曲率较大的边缘点称之为角点。<br>
1.Moravec算子<br>
Moravec是一种基于灰度方差的角点检测算子，该算子计算图像中某个像素点沿着水平、垂直、对角线、反对角线四个方向的灰度方差，将其中的最小值选为该像素点的兴趣值，再通过局部非极大值抑制来检测是否为角点。</p>
<ol>
<li>一般选取邻域大小为5x5，对于一个像素点分别计算4个灰度的方差，并选其中最小的值作为像素的代表值</li>
<li>选取一个经验阈值，筛选出一些代表值作为候选特征值</li>
<li>设定一个邻域大小，利用NMS算法，选出该邻域最大的特征值作为最终的特征值，去除多余的候选特征值<br>
优点：计算速度快<br>
缺点：对图像边缘和噪声敏感，并且有方向上的局限性<br>
2.Harris算子<br>
原理：直观的来看，角点处像素点的灰度值变换比较大。<br>
<img src="https://secrul.github.io/post-images/1599899093814.png" alt="" loading="lazy">，计算一个方形邻域灰度值的均值，如果是平坦区域均值变化很缓慢，如果是边界，均值只会在垂直于边界的方向上迅速变化。如果是角点，则均值对于任何方向变化都很大。<br>
Harris的改进：</li>
<li>Moravec算子只是考虑了8个方向，但是Harris用Taylor展开去近似任意方向。</li>
<li>Moravec算子使用的是方框，权值都是1，容易受到噪声影响，而harris使用高斯函数来加权。<br>
原理：<br>
对于每个像素点，规定了一个方形邻域，计算灰度差值：<br>
<img src="https://secrul.github.io/post-images/1599899224055.png" alt="" loading="lazy"><br>
<img src="https://secrul.github.io/post-images/1599899230495.png" alt="" loading="lazy"><br>
<img src="https://secrul.github.io/post-images/1599899253135.png" alt="" loading="lazy"><br>
<img src="https://secrul.github.io/post-images/1599899260785.png" alt="" loading="lazy"><br>
<img src="https://secrul.github.io/post-images/1599899273199.png" alt="" loading="lazy"><br>
<img src="https://secrul.github.io/post-images/1599899279572.png" alt="" loading="lazy"><br>
<img src="https://secrul.github.io/post-images/1599899286722.png" alt="" loading="lazy"><br>
算法流程：</li>
<li>利用水平，竖直差分算子对图像的每个像素进行滤波以求得Ix,Iy,进而求得M中的四个元素的值</li>
<li>对M的四个元素进行高斯平滑滤波，为的是消除一些不必要的孤立点和凸起，得到新的矩阵M</li>
<li>接下来利用M计算对应每个像素的角点响应函数R</li>
<li>局部极大值抑制，同时选取其极大值</li>
<li>在矩阵R中，同时满足R(i,j)大于一定阈值threshold和R(i,j)是某领域内的局部极大值，则被认为是角点<br>
优点：旋转不变性<br>
缺点：计算量大，尺度敏感<br>
3.SUSAN算子<br>
<img src="https://secrul.github.io/post-images/1599899766863.png" alt="" loading="lazy"><br>
圆形邻域，设定一个阈值，根据圆心和邻域内像素值灰度的插值确定模板填充0或者1.<br>
然后计算圆形邻域的的重心以及朝向。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simulated and Unsupervised ]]></title>
        <id>https://secrul.github.io/post/simulated-and-unsupervised/</id>
        <link href="https://secrul.github.io/post/simulated-and-unsupervised/">
        </link>
        <updated>2020-09-08T08:37:35.000Z</updated>
        <content type="html"><![CDATA[<p>由于数据集标注很昂贵，一般的合成图像和真实图像之间存在着一个差异，而这个差异可能会影响一些任务的正常进行。作者提出了SimGan来缩小合成图像和真实图像之间的差距。<br>
SimGan可以看作是一个GAN + Discriminator<br>
<img src="https://secrul.github.io/post-images/1599827658256.png" alt="" loading="lazy"><br>
SimGan由两部分组成，由R(Refiner network) 和 D(Discriminator network)组成。和传统GAN不同的，它的input是合成图片(synthetic image)，而不是random noise，经过Refiner之后产生refined image，然后再将real image和refined image送入Discriminator。<br>
创新点：</p>
<ol>
<li>self-regularization loss用来进一步保证refined images和synthetic images注释信息不变</li>
<li>local adversarial loss是为了避免GAN中的Discriminator过强导致生成假的图片，让D网络对图片的局部进行损失计算，而不是整张图片直接计算，这样也使得每张图片有多个损失函数值（multiple local adversarial losses per image</li>
<li>using a history of refined images 是为了保证D网络的稳定性，使用历史记录的refined图片更新D，而不仅仅是传统GAN网络每次提供的图片来更新。<br>
Refiner network<br>
<img src="https://secrul.github.io/post-images/1599967741582.png" alt="" loading="lazy"><br>
公式一是R网络的损失函数，但只给出了目标，前半部分是保证refined图片尽可能地像真实的图片，第二部分是保证refined图片保留原有的注释信息<br>
<img src="https://secrul.github.io/post-images/1599967807766.png" alt="" loading="lazy"><br>
关于D的损失函数，通过最小化该函数实现目标，第一部分是希望refined图片尽可能真，第二部分希望未标注真实图片输入后值尽可能小。这个的loss与原始GAN的D的loss刚好相反，原始GAN的D希望real图片尽可能为真，generated图片尽可能为假。也许是这样才能保证refined图片更加真实<br>
<img src="https://secrul.github.io/post-images/1599967925489.png" alt="" loading="lazy"><br>
R网络具体的损失函数，第一部分通过最小化该函数，利用已经训练过的D来更新R网络，保证refined图片与原始合成图片不同；第二部分是自动正则化部分（self-regularization）在像素级（进行特征提取后）确保refinged的图片与原始synthetic图片的注释信息不变。<br>
局部对抗损失函数：<br>
本文中提出在训练过程中Refiner网络经常会局限于图片的某一特征，重点放在一个区域来欺骗D网络，忽视了整张图片的特征。因此提出了local adversarial loss保证refined图片的每一个区域都应该趋向于真实的图片。<br>
<img src="https://secrul.github.io/post-images/1599969140623.png" alt="" loading="lazy"><br>
具体来说，并不是定义一个全局的判别器，而是定义一个判别器去分别判别图片的每一个区域。D网络输出WxH个区域的值，然后将每个区域进行交叉熵计算，并进行累加，从而提高整个网络的性能。<br>
<strong>使用一系列使用过的refined图片来更新Discriminator</strong><br>
在对抗训练中存在着另外一个问题就是D网络只会注意到最新的refined图片，这会导致一是整个训练的发散；二是R网络会生成重复的图片，但是D网络已经忘记了。对于判别器而言，任何时候R网络生成的图片都应该判别为假，基于此，本文提出了用之前训练过的refined图片来再次更新Discriminator。<br>
作者通过为之前生成的refined图片设置一缓存区，B为缓存区图片的数量，b为每次batch的大小。在每轮D的训练过程中，使用b/2的当前生成的图片和b/2的缓存区的图片来计算loss，更新参数。训练过程始终保持B的数量固定，每次训练结束用随机的b/2的新生成的refined图片来替换缓存区的图片。具体如下图所示。<br>
<img src="https://secrul.github.io/post-images/1599969540195.png" alt="" loading="lazy"></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[pix2pix]]></title>
        <id>https://secrul.github.io/post/pix2pix/</id>
        <link href="https://secrul.github.io/post/pix2pix/">
        </link>
        <updated>2020-09-08T08:37:07.000Z</updated>
        <content type="html"><![CDATA[<p>贡献：</p>
<ol>
<li>
<p>提出一个条件cGan来实现pix2pix的图像映射</p>
</li>
<li>
<p>损失函数Gan+L1</p>
</li>
<li>
<p>引入U-Net结构<br>
一般的图像处理任务都是将一幅图像转换为另一个相同结构的对应图像。如灰度图、彩色图之间的转换，图像去雾等。而对应的loss函数一般都是缩小结果图域GT图之间的L1误差，或者L2误差，但是这样会导致结果图比较模糊。这类问题实质都是从像素到像素的映射，作者就基于Gan提出了一个通用的方法——PatchGan，来解决这一类问题，得到比较清晰的结果图像。<br>
<img src="https://secrul.github.io/post-images/1600064839623.png" alt="" loading="lazy"><br>
pix2pix是cGan，即条件Gan的一种，普通Gan是基于一个随机的噪声来生成图像，而cGan可以添加条件信息来指导图像的生成。具体思路如下图所示：<br>
<img src="https://secrul.github.io/post-images/1600065209785.png" alt="" loading="lazy"><br>
生成器G输入有两部分组成，条件输入x和随机噪声z(z用来扩充产生结果的多样性)，输入伪造的图像G(x,z)，而判别器D的目标就是区分开真实图像{x,y}和伪造图像{x,G(x,y)}，一般Gan的判别器只知道产生结果G(x),但是条件Gan的判别器D也知道条件输入x的存在。<br>
目标函数如下：<br>
<img src="https://secrul.github.io/post-images/1600066190128.png" alt="" loading="lazy"> 和普通Gan的目标类似，不过是加上了条件输入x。作者还发现附加上一个L2损失函数是有好处的，可以使得图像更清晰，与GT更接近，但是在论文中采用的是L1损失函数。<br>
<img src="https://secrul.github.io/post-images/1600066731426.png" alt="" loading="lazy"><br>
Gan的目标就是：<br>
<img src="https://secrul.github.io/post-images/1600066953438.png" alt="" loading="lazy"></p>
<p>而在生成器网络结构上也有改进：<br>
<img src="https://secrul.github.io/post-images/1600067378297.png" alt="" loading="lazy"><br>
参考了UNet结构<br>
判别器采用PatchGAN，PatchGAN对输入图像的每个区域（patch）都输出一个预测概率值，相当于从判断输入是真还是假演变成判断输入的N*N大小区域是真还是假。举个例子，假设判别器的输入是1∗6∗256∗2561∗6∗256∗256，N设置为8，判别器的输出大小是1∗1∗32∗321∗1∗32∗32，其中32∗32大小的输出中的每个值都表示输入中对应8∗88∗8区域是真实的概率。<br>
普通Gan的判别器是对于整幅图像打一个分，patchGan简单理解就是分块预测属于正样本的概率，分块打分，然后进行加权投票。这样可以反向追踪哪些区域被激活，适用于一些高分辨率，高细节的图像领域。</p>
<p>优点：pix2pix巧妙的利用了GAN的框架来为“Image-to-Image translation”的一类问题提供了通用框架。利用U-Net提升细节，并且利用PatchGAN来处理图像的高频部分。<br>
优点：pix2pix巧妙的利用了GAN的框架来为“Image-to-Image translation”的一类问题提供了通用框架。利用U-Net提升细节，并且利用PatchGAN来处理图像的高频部分。<br>
缺点：训练需要大量的成对图片，比如白天转黑夜，则需要大量的同一个地方的白天和黑夜的照片。</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[aspp]]></title>
        <id>https://secrul.github.io/post/aspp/</id>
        <link href="https://secrul.github.io/post/aspp/">
        </link>
        <updated>2020-09-08T08:36:49.000Z</updated>
        <content type="html"><![CDATA[<p>a表示Atrous，意思是空洞。spp就是 spatial pyramid pooling 。论文中提出的结构就是：<br>
** 将spp-Net中的pooling操作，改为了空洞卷积，来进行采样操作。**<br>
论文贡献：</p>
<ol>
<li>使用空洞卷积来进行上采样来进行密集的预测任务。空洞卷积可以在不增加参数量的基础上增大filter的感受野，从而可以得到更多的语义信息。</li>
<li>空洞空间金字塔池化结构（ASPP）从而以多尺寸来分割目标物体。通过不同sample rates的filters及不同大小的感受野，来获得多尺寸下的语义信息</li>
<li>结合DCNN与概率模型提高物体的检测边界。DCNNs+CRF</li>
</ol>
<p>DCNN应用于分割的三个挑战：</p>
<ol>
<li>feature 分辨率的减少</li>
<li>不同尺寸的目标物</li>
<li>由于DCNN的不变性导致分割边界的不精确。<br>
提出的三个对策：</li>
<li>将DCNN最后几个maxpooling去掉，在后续的卷积层中添加更高sample rate的空洞卷积。结合空洞卷积与双线性插值将feature map 还原为原图大小</li>
<li>一个标准方式是将图片处理成相同的尺寸，然后融合特征与score maps，但会引入大量的计算。受到空间金字塔池化启发，对feature map以多rates 进行卷积，增强了感受野，该文对一张图片平行的使用不同sample rate 的空洞卷积层（ASPP）</li>
<li>一个对应物体中心的分类器，要求对空间的一些旋转等变换有类别不变性，一种方式是在进行最终的分割预测时通过跳跃连接不同层的 features 来获得潜在信息。本文，作者是用了一个全连接的CRF来改善模型对边界的分割<br>
DeepLab system 的优势：</li>
<li>速度</li>
<li>准确率</li>
<li>简单<br>
空洞卷积(膨胀卷积)：<br>
空洞卷积可以使用较少的参数，获取较大的感受野<br>
<img src="https://secrul.github.io/post-images/1599734881629.png" alt="" loading="lazy"><br>
一维空洞卷积</li>
</ol>
<p><img src="https://secrul.github.io/post-images/1599734953629.png" alt="" loading="lazy"><br>
可以看到同样的九个参数，利用空洞卷积获得了较大的感受野<br>
<img src="https://secrul.github.io/post-images/1599735011836.png" alt="" loading="lazy"><br>
空洞中用0填充</p>
<p><strong>全连接CRF</strong><br>
DCNN中的一个固有弊端是<strong>位置的准确性</strong>与<strong>分类的效果</strong>之间的矛盾， 带有池化层越深的模型其分类效果越好，但 持续增加的不变性与较大的感受野只会使结果更加平滑，但无法使边界更加分明。<br>
针对边界问题的方法，一是可以利用网络中多层feature map以便更好的进行边界分割。另一种方法是可以引入超像素表示，将边界任务交给较低级的分割方法处理。该文结合DCNN的识别能力与全连接CRF较高的分界效果。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[cycleGan]]></title>
        <id>https://secrul.github.io/post/cyclegan/</id>
        <link href="https://secrul.github.io/post/cyclegan/">
        </link>
        <updated>2020-09-08T01:18:59.000Z</updated>
        <content type="html"><![CDATA[<p>主要贡献：<br>
提出了在没有成对训练数据的基础之上的图像到图像的映射网络——cycleGan<br>
主要方法就是一个循环结构，这可能也是cycle的由来。<br>
<img src="https://secrul.github.io/post-images/1599528888099.png" alt="" loading="lazy"><br>
从X到Y的映射，G是生成器，Dy是对应的判别器。从Y到X的映射中F是生成器，Dx是对应判别器。通过这个循环结构建立一个双射，间接构成成对的训练数据。<br>
<img src="https://secrul.github.io/post-images/1599529086912.png" alt="" loading="lazy"><br>
可以看到，由X-&gt;Y的映射中，由G生成Y的一个估计值，再通过F将这个估计值作为输入，生成X的估计值X',我们将X和X'的误差作为衡量Cycle Consistency的一个损失函数。同理定义Y和Y'的Cycle Consistency的损失函数，这样子确保是一个双射。<br>
损失函数：<br>
<img src="https://secrul.github.io/post-images/1599531067331.png" alt="" loading="lazy"><br>
前一项是两个Gan的损失函数，后一项是Cycle Consistency的损失函数。<br>
<img src="https://secrul.github.io/post-images/1599531204459.png" alt="" loading="lazy"><br>
对于每一个Gan来说，损失函数不采用对数，而是采用L2范数，论文中这样更稳定，结果更好</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing and Understanding Convolutional Networks]]></title>
        <id>https://secrul.github.io/post/visualizing-and-understanding-convolutional-networks/</id>
        <link href="https://secrul.github.io/post/visualizing-and-understanding-convolutional-networks/">
        </link>
        <updated>2020-09-06T08:56:09.000Z</updated>
        <content type="html"><![CDATA[<p><img src="https://secrul.github.io/post-images/1599391324752.png" alt="" loading="lazy"><br>
提出了一种CNN的可视化方法。虽然CNN很搞笑，但是始终是一个黑盒子，我们提出一个模型只能通过试错来进行。可视化后可以帮助理解CNN，也可以借此来进一步优化一些CNN的结构。<br>
思路就是CNN网络从一幅图像得到一个向量后，在逆回去得到一幅图像。怎么逆回去的：</p>
<ol>
<li>逆池化：在最大值池化中记录每个最大值的位置，然后将这些最大值填回到原始位置，就得到一个稀疏的大尺寸向量</li>
<li>逆ReLU：还是采用ReLU</li>
<li>逆卷积：还是采用正向卷进的卷积核，将之转置后进行卷积操作。一个像素点是很多像素点的加权和，按照权重退还给每个像素点<br>
论文中基于AlexNet。通过可视化发现：<br>
第一层得到了一些高频信息。<br>
第二层得到了角落、边缘信息<br>
第三层得到了复杂的纹理信息<br>
第四层获得了局部的类别信息，如狗的脸，鸟的腿<br>
第五层得到了完整的高层信息<br>
通过调小卷积核的大小，减小步距，可以优化网络</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Deconvolution Network for Semantic Segmentation]]></title>
        <id>https://secrul.github.io/post/learning-deconvolution-network-for-semantic-segmentation/</id>
        <link href="https://secrul.github.io/post/learning-deconvolution-network-for-semantic-segmentation/">
        </link>
        <updated>2020-09-05T01:35:04.000Z</updated>
        <content type="html"><![CDATA[<p><img src="https://secrul.github.io/post-images/1599297360690.png" alt="" loading="lazy"><br>
FCN缺点：</p>
<ol>
<li>感受野为固定大小；物体大于或小于感受野会被 fragmented or mislabeled，大物体，分割不连续；小物体被忽视,如下图所示．虽然FCN加入了skip architecturer,但是这并不是根本的解决方案，根本问题–详细的边界和语义</li>
<li>输入反卷积层的label map太粗糙，反卷积过程太简单，使得物体的细节信息丢失或被平滑<br>
本文贡献：</li>
<li>学习了一个多层的deconvolution network,它由deconvolution,unpooling和ReLU层组成．</li>
<li>将训练好network应用在每个目标候选区域(object proposal),来获得instance-wise segmentions,再将这些单个的分割结果结合起来组成最终的语义分割结果；这解决了原始的FCN存在的物体尺度问题，能够识别细小的结构</li>
<li>将FCN的结果和本文的结果结合起来，获得目前最好的语义分割结果</li>
</ol>
]]></content>
    </entry>
</feed>